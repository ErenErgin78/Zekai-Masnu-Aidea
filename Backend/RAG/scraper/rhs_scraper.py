from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import json
import os
from datetime import datetime

class RHSScraper:
    def __init__(self):
        # Chrome seÃ§eneklerini ayarla
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # TarayÄ±cÄ±yÄ± gÃ¶sterme (isterseniz kaldÄ±rabilirsiniz)
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.base_url = "https://www.rhs.org.uk"
        self.advice_url = f"{self.base_url}/advice/advice-search"
        self.wait = WebDriverWait(self.driver, 10)
        
        # Veri kaydetme klasÃ¶rÃ¼ - mutlak yol kullan
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.output_dir = os.path.join(script_dir, "rhs_scraped_data")
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸ“ Veri klasÃ¶rÃ¼: {self.output_dir}")
        
    def get_categories(self):
        """TÃ¼m kategorileri al"""
        print("Kategoriler alÄ±nÄ±yor...")
        self.driver.get(self.advice_url)
        time.sleep(3)
        
        categories = []
        try:
            # Kategori checkbox'larÄ±nÄ± bul
            checkboxes = self.driver.find_elements(By.CSS_SELECTOR, 
                "filter-list-item input[type='checkbox']:not([disabled])")
            
            for checkbox in checkboxes:
                checkbox_id = checkbox.get_attribute('id')
                label = self.driver.find_element(By.CSS_SELECTOR, 
                    f"label[for='{checkbox_id}'] .form-checkbox__text")
                
                category_text = label.text
                category_name = category_text.split('(')[0].strip()
                
                categories.append({
                    'id': checkbox_id,
                    'name': category_name,
                    'full_text': category_text
                })
                
            print(f"{len(categories)} kategori bulundu.")
            return categories
            
        except Exception as e:
            print(f"Kategori alma hatasÄ±: {e}")
            return []
    
    def click_category(self, category_id):
        """Belirli bir kategoriye tÄ±kla"""
        try:
            # Label'Ä± bul ve ona tÄ±kla
            label = self.wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, f"label[for='{category_id}']"))
            )
            
            # SayfayÄ± scroll et (gÃ¶rÃ¼nÃ¼r hale getir)
            self.driver.execute_script("arguments[0].scrollIntoView(true);", label)
            time.sleep(0.5)
            
            # JavaScript ile tÄ±kla
            self.driver.execute_script("arguments[0].click();", label)
            
            # SayfanÄ±n gÃ¼ncellenmesini bekle - loading sÄ±nÄ±fÄ±nÄ±n kaybolmasÄ±nÄ± bekle
            time.sleep(2)
            
            # Loading animasyonunun bitmesini kontrol et
            try:
                self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "advice-list .gl-view"))
                )
                print("  âœ“ Kategori seÃ§ildi ve makaleler yÃ¼klendi")
            except TimeoutException:
                print("  âš  Makaleler yÃ¼klenirken zaman aÅŸÄ±mÄ±")
            
            time.sleep(1)  # Ek gÃ¼venlik bekleme
            
            return True
        except Exception as e:
            print(f"  âœ— Kategori tÄ±klama hatasÄ±: {e}")
            # Alternatif yÃ¶ntem: DoÄŸrudan checkbox'a tÄ±kla
            try:
                checkbox = self.driver.find_element(By.ID, category_id)
                self.driver.execute_script("arguments[0].checked = true;", checkbox)
                self.driver.execute_script("arguments[0].dispatchEvent(new Event('change', { bubbles: true }));", checkbox)
                time.sleep(3)
                print("  âœ“ Alternatif yÃ¶ntemle kategori seÃ§ildi")
                return True
            except Exception as e2:
                print(f"  âœ— Alternatif yÃ¶ntem de baÅŸarÄ±sÄ±z: {e2}")
                return False
    
    def get_article_links(self):
        """Sayfadaki tÃ¼m makale linklerini al"""
        article_links = []
        try:
            # Sayfa yÃ¼klenene kadar bekle - loading sÄ±nÄ±fÄ± kaybolsun
            self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "advice-list"))
            )
            
            # Loading animasyonunun bitmesini bekle
            time.sleep(2)
            
            # Makalelerin yÃ¼klendiÄŸinden emin ol
            try:
                self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "advice-list-item"))
                )
            except TimeoutException:
                print("  HiÃ§ makale bulunamadÄ± (timeout)")
                return []
            
            # INFINITE SCROLL - TÃ¼m makaleleri yÃ¼kle
            print("  â³ TÃ¼m makaleler yÃ¼kleniyor (infinite scroll)...")
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            articles_count = 0
            no_change_count = 0
            
            while True:
                # SayfanÄ±n en altÄ±na scroll yap
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # Yeni iÃ§eriÄŸin yÃ¼klenmesi iÃ§in bekle
                
                # Yeni yÃ¼ksekliÄŸi kontrol et
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                
                # Makale sayÄ±sÄ±nÄ± kontrol et
                current_articles = len(self.driver.find_elements(By.CSS_SELECTOR, "advice-list-item"))
                
                if current_articles > articles_count:
                    print(f"    ğŸ“„ {current_articles} makale yÃ¼klendi...")
                    articles_count = current_articles
                    no_change_count = 0
                else:
                    no_change_count += 1
                
                # EÄŸer sayfa yÃ¼ksekliÄŸi deÄŸiÅŸmediyse veya 3 kez Ã¼st Ã¼ste deÄŸiÅŸim olmadÄ±ysa bitir
                if new_height == last_height or no_change_count >= 3:
                    print(f"  âœ“ TÃ¼m makaleler yÃ¼klendi (toplam: {articles_count})")
                    break
                
                last_height = new_height
                
                # GÃ¼venlik iÃ§in maksimum 50 scroll
                if no_change_count >= 50:
                    print("  âš  Maksimum scroll sayÄ±sÄ±na ulaÅŸÄ±ldÄ±")
                    break
            
            # En Ã¼ste geri dÃ¶n
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # TÃ¼m makale elementlerini bul
            article_items = self.driver.find_elements(By.CSS_SELECTOR, "advice-list-item")
            
            for item in article_items:
                try:
                    # Title'Ä± al
                    title_elem = item.find_element(By.CSS_SELECTOR, "h4.gl-view__title")
                    title = title_elem.text.strip()
                    
                    # Link'i al
                    link_elem = item.find_element(By.CSS_SELECTOR, "a.u-faux-block-link__overlay")
                    href = link_elem.get_attribute('href')
                    
                    # BoÅŸ title'larÄ± atla (test sayfalarÄ± vs.)
                    if href and title:
                        article_links.append({
                            'url': href if href.startswith('http') else f"{self.base_url}{href}",
                            'title': title
                        })
                except NoSuchElementException:
                    continue
            
            print(f"  âœ… {len(article_links)} makale bulundu.")
            return article_links
            
        except Exception as e:
            print(f"Makale linkleri alma hatasÄ±: {e}")
            return []
    
    def scrape_article(self, article_url, article_title):
        """Tek bir makaleyi scrape et"""
        try:
            self.driver.get(article_url)
            time.sleep(2)
            
            article_data = {
                'url': article_url,
                'title': article_title,
                'scraped_at': datetime.now().isoformat()
            }
            
            # Quick facts - dÃ¼z metin olarak
            try:
                quick_facts_div = self.driver.find_element(By.CSS_SELECTOR, 
                    "div.panel--border .content")
                article_data['quick_facts'] = quick_facts_div.text.strip()
            except NoSuchElementException:
                article_data['quick_facts'] = ""
            
            # Ana iÃ§erik bÃ¶lÃ¼mleri - temiz metin
            sections = []
            try:
                section_containers = self.driver.find_elements(By.CSS_SELECTOR, 
                    "div[id^='section-']")
                
                for section in section_containers:
                    try:
                        heading = section.find_element(By.TAG_NAME, 'h3').text
                        
                        # Ä°Ã§eriÄŸi temiz metin olarak al
                        content_elem = section.find_element(By.CSS_SELECTOR, '.content-steps')
                        content_text = content_elem.text.strip()
                        
                        # Resimleri al (eÄŸer varsa)
                        images = []
                        try:
                            img_elements = section.find_elements(By.CSS_SELECTOR, 'img')
                            for img in img_elements:
                                img_src = img.get_attribute('src')
                                img_alt = img.get_attribute('alt') or img.get_attribute('title')
                                if img_src and not img_src.endswith('ng-lazyloading'):
                                    images.append({
                                        'src': img_src,
                                        'alt': img_alt
                                    })
                        except:
                            pass
                        
                        sections.append({
                            'heading': heading,
                            'content': content_text,
                            'images': images
                        })
                    except NoSuchElementException:
                        continue
                        
            except NoSuchElementException:
                pass
            
            article_data['sections'] = sections
            
            # "See also" bÃ¶lÃ¼mÃ¼
            try:
                see_also = self.driver.find_element(By.CSS_SELECTOR, "div[id='section-200']")
                article_data['see_also'] = see_also.text.strip()
            except NoSuchElementException:
                article_data['see_also'] = ""
            
            # Metadata
            try:
                # Kategori label
                category = self.driver.find_element(By.CSS_SELECTOR, "h5.label").text
                article_data['category'] = category
            except NoSuchElementException:
                article_data['category'] = ""
            
            return article_data
            
        except Exception as e:
            print(f"  âŒ Makale scrape hatasÄ± ({article_title}): {e}")
            return None
    
    def save_data(self, data, filename):
        """Veriyi JSON olarak kaydet"""
        # KlasÃ¶rÃ¼n var olduÄŸundan emin ol
        os.makedirs(self.output_dir, exist_ok=True)
        
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            # Sadece kategori dosyalarÄ± iÃ§in mesaj gÃ¶ster
            if not filename.startswith('scraping_summary'):
                article_count = data.get('article_count', 0)
                print(f"  ğŸ’¾ Kaydedildi: {filename} ({article_count} makale)")
        except Exception as e:
            print(f"  âŒ KayÄ±t hatasÄ± ({filename}): {e}")
            print(f"     Denenen yol: {os.path.abspath(filepath)}")
    
    def scrape_all(self):
        """TÃ¼m kategorileri ve makaleleri scrape et"""
        start_time = datetime.now()
        
        print("\n" + "=" * 70)
        print("ğŸŒ± RHS ADVICE SCRAPER".center(70))
        print("=" * 70)
        
        categories = self.get_categories()
        
        all_data = {
            'scraped_at': start_time.isoformat(),
            'total_categories': len(categories),
            'categories': [],
            'statistics': {}
        }
        
        total_articles_scraped = 0
        successful_scrapes = 0
        failed_scrapes = 0
        
        for idx, category in enumerate(categories, 1):
            print(f"\nâ”Œ{'â”€' * 68}â”")
            print(f"â”‚ ğŸ“‚ [{idx}/{len(categories)}] {category['name']:<60} â”‚")
            print(f"â””{'â”€' * 68}â”˜")
            
            # Ana sayfaya dÃ¶n
            self.driver.get(self.advice_url)
            time.sleep(2)
            
            # Kategoriyi seÃ§
            if not self.click_category(category['id']):
                continue
            
            # Makale linklerini al
            article_links = self.get_article_links()
            
            category_data = {
                'name': category['name'],
                'id': category['id'],
                'article_count': len(article_links),
                'articles': []
            }
            
            # Her makaleyi scrape et
            category_success = 0
            category_failed = 0
            
            for art_idx, article in enumerate(article_links, 1):
                progress = f"[{art_idx}/{len(article_links)}]"
                title_truncated = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
                print(f"  {progress:>10} ğŸ“„ {title_truncated}")
                
                article_data = self.scrape_article(article['url'], article['title'])
                
                if article_data:
                    category_data['articles'].append(article_data)
                    category_success += 1
                    successful_scrapes += 1
                else:
                    category_failed += 1
                    failed_scrapes += 1
                    
                time.sleep(1)  # Kibar scraping iÃ§in bekleme
            
            total_articles_scraped += len(article_links)
            
            # Kategori Ã¶zeti
            print(f"\n  âœ… Kategori tamamlandÄ±: {category_success} baÅŸarÄ±lÄ±, {category_failed} baÅŸarÄ±sÄ±z")
            
            # Kategori verilerini kaydet
            safe_name = category['name'].replace('/', '_').replace(' ', '_').lower()
            self.save_data(category_data, f"category_{safe_name}.json")
            
            all_data['categories'].append({
                'name': category['name'],
                'article_count': len(category_data['articles']),
                'successful': category_success,
                'failed': category_failed
            })
        
        # Ä°statistikleri hesapla
        end_time = datetime.now()
        duration = end_time - start_time
        
        all_data['statistics'] = {
            'total_articles_found': total_articles_scraped,
            'successful_scrapes': successful_scrapes,
            'failed_scrapes': failed_scrapes,
            'success_rate': f"{(successful_scrapes / total_articles_scraped * 100):.2f}%" if total_articles_scraped > 0 else "0%",
            'duration': str(duration).split('.')[0],  # HH:MM:SS formatÄ±nda
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat()
        }
        
        # Genel Ã¶zet kaydet
        self.save_data(all_data, "scraping_summary.json")
        
        # Final Ã¶zet
        print("\n" + "=" * 70)
        print("âœ¨ SCRAPING TAMAMLANDI!".center(70))
        print("=" * 70)
        print(f"\nğŸ“Š Ä°STATÄ°STÄ°KLER:")
        print(f"   â”œâ”€ Toplam Kategori      : {len(categories)}")
        print(f"   â”œâ”€ Toplam Makale        : {total_articles_scraped}")
        print(f"   â”œâ”€ BaÅŸarÄ±lÄ± Scrape      : {successful_scrapes} âœ…")
        print(f"   â”œâ”€ BaÅŸarÄ±sÄ±z Scrape     : {failed_scrapes} âŒ")
        print(f"   â”œâ”€ BaÅŸarÄ± OranÄ±         : {all_data['statistics']['success_rate']}")
        print(f"   â””â”€ Toplam SÃ¼re          : {all_data['statistics']['duration']}")
        
        print(f"\nğŸ’¾ KAYDEDILEN DOSYALAR:")
        print(f"   â”œâ”€ Kategori JSON'larÄ±   : {len(categories)} dosya")
        print(f"   â”œâ”€ Ã–zet dosyasÄ±         : scraping_summary.json")
        print(f"   â””â”€ Konum                : {os.path.abspath(self.output_dir)}/")
        
        print(f"\nğŸ¯ EN Ã‡OK MAKALE Ä°Ã‡EREN KATEGORÄ°LER:")
        sorted_categories = sorted(all_data['categories'], key=lambda x: x['article_count'], reverse=True)[:5]
        for i, cat in enumerate(sorted_categories, 1):
            print(f"   {i}. {cat['name']:<30} : {cat['article_count']} makale")
        
        print("\n" + "=" * 70 + "\n")
    
    def close(self):
        """TarayÄ±cÄ±yÄ± kapat"""
        self.driver.quit()


# KullanÄ±m
if __name__ == "__main__":
    scraper = RHSScraper()
    try:
        scraper.scrape_all()
    finally:
        scraper.close()