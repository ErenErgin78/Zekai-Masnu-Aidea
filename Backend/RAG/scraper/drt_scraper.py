from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import os
import time
import re

class DRTScraper:
    def __init__(self):
        self.base_url = "https://www.drt.com.tr"
        self.main_url = "https://www.drt.com.tr/faydali-bilgiler/"
        
        # Selenium WebDriver ayarlarÄ±
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # TarayÄ±cÄ±yÄ± gizli modda Ã§alÄ±ÅŸtÄ±r
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # LoglarÄ± azalt
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def clean_filename(self, filename):
        """Dosya adÄ±nÄ± temizle ve geÃ§erli hale getir"""
        # TÃ¼rkÃ§e karakterleri dÃ¼zelt
        filename = filename.replace('Ã§', 'c').replace('ÄŸ', 'g').replace('Ä±', 'i')
        filename = filename.replace('Ã¶', 'o').replace('ÅŸ', 's').replace('Ã¼', 'u')
        filename = filename.replace('Ã‡', 'C').replace('Äž', 'G').replace('Ä°', 'I')
        filename = filename.replace('Ã–', 'O').replace('Åž', 'S').replace('Ãœ', 'U')
        
        # GeÃ§ersiz karakterleri kaldÄ±r
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        filename = filename.strip()
        
        # Uzunluk sÄ±nÄ±rlamasÄ±
        if len(filename) > 100:
            filename = filename[:100]
            
        return filename
    
    def get_article_urls_from_html(self, html_content):
        """HTML iÃ§eriÄŸinden makale URL'lerini Ã§Ä±kar"""
        soup = BeautifulSoup(html_content, 'html.parser')
        article_urls = []
        
        # 1. YÃ¶ntem: heading iÃ§indeki linkleri bul
        headings = soup.find_all('h2', class_='elementor-heading-title')
        for heading in headings:
            link = heading.find('a', href=True)
            if link:
                href = link['href']
                if href not in article_urls and 'drt.com.tr' in href:
                    article_urls.append(href)
        
        # 2. YÃ¶ntem: jet-listing-grid iÃ§indeki tÃ¼m a etiketleri
        listing_grid = soup.find('div', class_='jet-listing-grid')
        if listing_grid:
            links = listing_grid.find_all('a', href=True)
            for link in links:
                href = link['href']
                if href not in article_urls and 'drt.com.tr' in href and '/wp-content/' not in href:
                    article_urls.append(href)
        
        # 3. YÃ¶ntem: elementor-post baÅŸlÄ±klÄ± tÃ¼m a etiketleri
        post_links = soup.find_all('a', class_=lambda x: x and 'elementor-post' in x)
        for link in post_links:
            if 'href' in link.attrs:
                href = link['href']
                if href not in article_urls and 'drt.com.tr' in href:
                    article_urls.append(href)
        
        return list(set(article_urls))
    
    def wait_for_pagination(self):
        """Pagination yÃ¼klenene kadar bekle"""
        try:
            # Pagination item'larÄ±nÄ±n yÃ¼klenmesini bekle
            self.wait.until(
                EC.presence_of_element_located((By.CLASS_NAME, "jet-filters-pagination__item"))
            )
            time.sleep(1)  # JavaScript'in tam yÃ¼klenmesi iÃ§in ekstra sÃ¼re
            return True
        except TimeoutException:
            print("  âš  Pagination yÃ¼klenmedi")
            return False
    
    def has_next_button(self, debug=False):
        """Sonraki butonu var mÄ± kontrol et - Selenium ile"""
        try:
            # Ã–nce pagination'Ä±n yÃ¼klenmesini bekle
            self.wait_for_pagination()
            
            if debug:
                print("\n  ðŸ” BUTON TESPÄ°T DETAYLARI:")
                print("  " + "-" * 50)
            
            # TÃ¼m pagination item'larÄ±nÄ± bul
            items = self.driver.find_elements(By.CLASS_NAME, "jet-filters-pagination__item")
            
            if debug:
                print(f"  âœ“ Toplam {len(items)} pagination item bulundu")
                for idx, item in enumerate(items, 1):
                    classes = item.get_attribute('class')
                    data_value = item.get_attribute('data-value')
                    text = item.text.strip()
                    print(f"    [{idx}] class='{classes}' | data-value='{data_value}' | text='{text}'")
                print("  " + "-" * 50)
            
            # Sonraki butonunu ara
            for item in items:
                data_value = item.get_attribute('data-value')
                classes = item.get_attribute('class')
                
                if data_value == 'next' or 'next' in classes:
                    if debug:
                        print("  âœ“ SONUÃ‡: Sonraki butonu BULUNDU!\n")
                    return True
            
            if debug:
                print("  âœ— SONUÃ‡: Sonraki butonu YOK\n")
            return False
            
        except Exception as e:
            if debug:
                print(f"  âœ— Hata: {e}\n")
            return False
    
    def click_next_button(self):
        """Sonraki butonuna tÄ±kla"""
        try:
            # Sonraki butonunu bul
            next_button = self.driver.find_element(
                By.CSS_SELECTOR, 
                ".jet-filters-pagination__item[data-value='next']"
            )
            
            # Butona kaydÄ±r (ortaya gelsin)
            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)
            time.sleep(1)
            
            # YÃ¶ntem 1: JavaScript ile tÄ±kla (en gÃ¼venilir)
            try:
                self.driver.execute_script("arguments[0].click();", next_button)
                print("  âœ“ Sonraki butonuna tÄ±klandÄ± (JavaScript)")
            except Exception as e1:
                print(f"  âš  JavaScript tÄ±klama baÅŸarÄ±sÄ±z, alternatif yÃ¶ntem deneniyor...")
                
                # YÃ¶ntem 2: Normal tÄ±klama
                try:
                    next_button.click()
                    print("  âœ“ Sonraki butonuna tÄ±klandÄ± (Normal)")
                except Exception as e2:
                    print(f"  âš  Normal tÄ±klama baÅŸarÄ±sÄ±z, ActionChains deneniyor...")
                    
                    # YÃ¶ntem 3: ActionChains ile tÄ±kla
                    from selenium.webdriver.common.action_chains import ActionChains
                    actions = ActionChains(self.driver)
                    actions.move_to_element(next_button).click().perform()
                    print("  âœ“ Sonraki butonuna tÄ±klandÄ± (ActionChains)")
            
            # AJAX yÃ¼klemesinin bitmesini bekle
            print("  â³ Sayfa yÃ¼kleniyor...")
            time.sleep(4)  # AJAX iÃ§in daha uzun sÃ¼re
            
            # Yeni iÃ§eriÄŸin yÃ¼klenmesini bekle
            self.wait_for_pagination()
            
            return True
            
        except NoSuchElementException:
            print("  âœ— Sonraki butonu bulunamadÄ±")
            return False
        except Exception as e:
            print(f"  âœ— TÄ±klama hatasÄ±: {e}")
            return False
    
    def get_all_article_urls(self):
        """TÃ¼m sayfalardaki makale URL'lerini topla - Selenium ile"""
        all_urls = []
        page_num = 1
        
        print("Makale URL'leri toplanÄ±yor...\n")
        
        # Ä°lk sayfayÄ± aÃ§
        print(f"--- Sayfa {page_num} ---")
        print(f"URL aÃ§Ä±lÄ±yor: {self.main_url}")
        self.driver.get(self.main_url)
        
        # Pagination yÃ¼klenmesini bekle
        self.wait_for_pagination()
        
        # Ä°lk sayfadaki makaleleri al
        current_html = self.driver.page_source
        page_urls = self.get_article_urls_from_html(current_html)
        all_urls.extend(page_urls)
        print(f"âœ“ {len(page_urls)} makale bulundu")
        
        # Sonraki butonu var mÄ± kontrol et
        has_next = self.has_next_button(debug=True)
        print(f"Sonraki butonu: {'âœ“ VAR' if has_next else 'âœ— YOK'}")
        
        # Sonraki butonu oldukÃ§a devam et
        while has_next:
            page_num += 1
            print(f"\n--- Sayfa {page_num} ---")
            
            # Sonraki butonuna tÄ±kla
            if self.click_next_button():
                # Makaleleri al
                current_html = self.driver.page_source
                page_urls = self.get_article_urls_from_html(current_html)
                
                if page_urls:
                    all_urls.extend(page_urls)
                    print(f"âœ“ {len(page_urls)} makale bulundu")
                else:
                    print("âœ— Bu sayfada makale bulunamadÄ±")
                
                # Sonraki butonu hala var mÄ±?
                has_next = self.has_next_button(debug=True)
                print(f"Sonraki butonu: {'âœ“ VAR' if has_next else 'âœ— YOK'}")
                
                if not has_next:
                    print("\nâ†’ Sonraki butonu kayboldu, pagination tamamlandÄ±!")
                    break
            else:
                print("âœ— Sonraki buton tÄ±klanamadÄ±, duruluyor...")
                break
            
            # Sonsuz dÃ¶ngÃ¼ Ã¶nleme
            if page_num >= 50:
                print("\nâš  Maksimum sayfa sayÄ±sÄ±na ulaÅŸÄ±ldÄ± (50), duruluyor...")
                break
        
        # TekilleÅŸtir
        unique_urls = list(set(all_urls))
        print(f"\n{'=' * 60}")
        print(f"âœ“ Toplam {len(unique_urls)} benzersiz makale bulundu")
        print(f"{'=' * 60}")
        return unique_urls
    
    def scrape_article_content(self, article_url):
        """Bir makalenin iÃ§eriÄŸini Ã§ek"""
        try:
            self.driver.get(article_url)
            time.sleep(2)  # Sayfa yÃ¼klenmesini bekle
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # BaÅŸlÄ±ÄŸÄ± al
            title_element = soup.find('h1') or soup.find('h2', class_='elementor-heading-title')
            title = title_element.get_text().strip() if title_element else "BaÅŸlÄ±ksÄ±z Makale"
            
            # Ä°Ã§eriÄŸi al
            content_div = soup.find('div', class_='elementor-widget-theme-post-content')
            if not content_div:
                content_div = soup.find('div', class_='elementor-widget-text-editor')
            
            content_text = ""
            if content_div:
                paragraphs = content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'])
                for p in paragraphs:
                    text = p.get_text().strip()
                    if text:
                        content_text += text + "\n\n"
            
            # Yedek yÃ¶ntem
            if not content_text.strip():
                paragraphs = soup.find_all('p')
                for p in paragraphs:
                    text = p.get_text().strip()
                    if text and len(text) > 20:
                        content_text += text + "\n\n"
            
            return title, content_text.strip()
            
        except Exception as e:
            print(f"    âœ— Makale Ã§ekme hatasÄ±: {e}")
            return None, None
    
    def save_article_to_file(self, title, content, index, total):
        """Makaleyi txt dosyasÄ±na kaydet"""
        if not content:
            return False
        
        clean_title = self.clean_filename(title)
        filename = f"{index:03d}_{clean_title}.txt"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"BaÅŸlÄ±k: {title}\n")
                f.write(f"=" * 50 + "\n\n")
                f.write(content)
            
            print(f"    âœ“ Kaydedildi: {filename}")
            return True
            
        except Exception as e:
            print(f"    âœ— Kaydetme hatasÄ±: {e}")
            return False
    
    def run(self):
        """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
        print("=" * 60)
        print("DRT Scraper - Selenium ile Pagination")
        print("=" * 60 + "\n")
        
        try:
            # Ã‡Ä±ktÄ± klasÃ¶rÃ¼ oluÅŸtur
            if not os.path.exists('drt_makaleler'):
                os.makedirs('drt_makaleler')
            os.chdir('drt_makaleler')
            
            # TÃ¼m makale URL'lerini al
            article_urls = self.get_all_article_urls()
            
            if not article_urls:
                print("âœ— HiÃ§ makale bulunamadÄ±!")
                return
            
            print(f"\n{'=' * 60}")
            print(f"Makale iÃ§erikleri indiriliyor...")
            print(f"{'=' * 60}\n")
            
            # Her makaleyi iÅŸle
            success_count = 0
            for i, url in enumerate(article_urls, 1):
                print(f"[{i}/{len(article_urls)}] {url}")
                
                title, content = self.scrape_article_content(url)
                
                if title and content:
                    if self.save_article_to_file(title, content, i, len(article_urls)):
                        success_count += 1
                else:
                    print(f"    âœ— Ä°Ã§erik Ã§ekilemedi")
                
                time.sleep(1)
            
            print(f"\n{'=' * 60}")
            print(f"Ä°ÅŸlem TamamlandÄ±!")
            print(f"BaÅŸarÄ±lÄ±: {success_count}/{len(article_urls)}")
            print(f"KlasÃ¶r: drt_makaleler/")
            print(f"{'=' * 60}")
            
        finally:
            # TarayÄ±cÄ±yÄ± kapat
            print("\nTarayÄ±cÄ± kapatÄ±lÄ±yor...")
            self.driver.quit()

if __name__ == "__main__":
    scraper = DRTScraper()
    scraper.run()