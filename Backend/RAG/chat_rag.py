# chat_rag.py - Token Optimizasyonlu
from rag_processor import RAGProcessor
from gemini_client import GeminiClient
import os
import warnings

warnings.filterwarnings('ignore')

class RAGChatbot:
    def __init__(self, max_sources=3, max_context_length=3000):
        print("ğŸ¤– RAG Chatbot baÅŸlatÄ±lÄ±yor...")
        self.rag_processor = RAGProcessor()
        self.gemini_client = GeminiClient()
        self.conversation_history = []
        self.max_sources = max_sources  # Token tasarrufu iÃ§in
        self.max_context_length = max_context_length  # Context token limiti
        print("âœ… RAG Chatbot hazÄ±r!")
    
    def query(self, question: str, num_sources: int = None):
        """Soru sor ve cevap al
        
        Args:
            question: KullanÄ±cÄ± sorusu
            num_sources: KaÃ§ kaynak kullanÄ±lacak (None ise default kullanÄ±lÄ±r)
        """
        print(f"\nğŸ“ Soru: {question}")
        
        # Kaynak sayÄ±sÄ±nÄ± belirle
        k = num_sources if num_sources is not None else self.max_sources
        
        # Benzer iÃ§erikleri bul
        similar_docs = self.rag_processor.search_similar(question, k=k)
        
        if not similar_docs:
            print("âš ï¸ Ä°lgili iÃ§erik bulunamadÄ±")
            return "ÃœzgÃ¼nÃ¼m, bu konuyla ilgili kaynaklarÄ±mda yeterli bilgi bulamadÄ±m. LÃ¼tfen baÅŸka bir ÅŸekilde sormayÄ± deneyin veya farklÄ± bir konu hakkÄ±nda soru sorun.", []
        
        print(f"âœ… {len(similar_docs)} kaynak bulundu")
        
        # Context oluÅŸtur - token limiti ile
        context_parts = []
        current_length = 0
        
        for doc in similar_docs:
            content = doc.page_content
            content_length = len(content)
            
            # Token limiti kontrolÃ¼ (yaklaÅŸÄ±k: 1 token â‰ˆ 4 karakter)
            if current_length + content_length > self.max_context_length:
                # Kalan alanÄ± kullan
                remaining = self.max_context_length - current_length
                if remaining > 100:  # En az 100 karakter ekle
                    context_parts.append(content[:remaining])
                break
            
            context_parts.append(content)
            current_length += content_length
        
        context = "\n\n".join(context_parts)
        
        print(f"ğŸ“Š Context uzunluÄŸu: {len(context)} karakter (~{len(context)//4} token)")
        
        # Gemini'ye sor
        print("ğŸ¤” Gemini cevap Ã¼retiyor...")
        response = self.gemini_client.generate_response(question, context)
        
        # KonuÅŸma geÃ§miÅŸine ekle
        self.conversation_history.append({
            "soru": question,
            "cevap": response,
            "kaynak_sayisi": len(similar_docs)
        })
        
        return response, similar_docs
    
    def show_sources(self, sources):
        """KaynaklarÄ± gÃ¶ster (tekrarsÄ±z + chunk sayÄ±sÄ±)"""
        if sources:
            # Kaynak isimlerine gÃ¶re chunk sayÄ±sÄ±nÄ± say
            source_counts = {}
            for doc in sources:
                source = doc.metadata.get('source', 'Bilinmeyen')
                source_name = os.path.basename(source)
                source_counts[source_name] = source_counts.get(source_name, 0) + 1
            
            print("\nğŸ“š KAYNAKLAR:")
            for i, (source_name, count) in enumerate(source_counts.items(), 1):
                if count > 1:
                    print(f"  {i}. {source_name} ({count} bÃ¶lÃ¼m)")
                else:
                    print(f"  {i}. {source_name}")

def setup_vector_store():
    """VektÃ¶r veritabanÄ±nÄ± kontrol et"""
    vector_store_path = "vector_store"
    sqlite_file = os.path.join(vector_store_path, "chroma.sqlite3")
    
    if os.path.exists(sqlite_file):
        print(f"âœ… VektÃ¶r veritabanÄ± bulundu: {sqlite_file}")
        return True
    else:
        print(f"âŒ VektÃ¶r veritabanÄ± bulunamadÄ±: {sqlite_file}")
        return False

def main():
    print("=" * 60)
    print("ğŸŒ± ORGANÄ°K TARIM ASISTANI ğŸŒ±".center(60))
    print("=" * 60)
    print()
    
    # VektÃ¶r veritabanÄ±nÄ± kontrol et
    if not setup_vector_store():
        print("\nğŸ”¥ PDF'ler iÅŸleniyor, lÃ¼tfen bekleyin...")
        rag_processor = RAGProcessor()
        success = rag_processor.load_and_process_pdfs()
        if success:
            print("âœ… Ä°ÅŸlem tamamlandÄ±!")
        else:
            print("âŒ PDF iÅŸleme baÅŸarÄ±sÄ±z! RAG devre dÄ±ÅŸÄ±.")
            return
    
    # Chatbot'u baÅŸlat (token optimizasyonlu)
    chatbot = RAGChatbot(max_sources=3, max_context_length=3000)
    
    print("\nğŸ’¬ Merhaba! Organik tarÄ±m hakkÄ±ndaki sorularÄ±nÄ± cevaplayabilirim.")
    print("ğŸ” Ä°pucu: 'Ã§Ä±kÄ±ÅŸ', 'exit' veya 'quit' yazarak Ã§Ä±kabilirsin.\n")
    print("=" * 60)
    
    while True:
        try:
            # KullanÄ±cÄ±dan soru al
            print("\n" + "â”€" * 60)
            user_input = input("ğŸ™‹ Sen: ").strip()
            
            # Ã‡Ä±kÄ±ÅŸ kontrolÃ¼
            if user_input.lower() in ['Ã§Ä±kÄ±ÅŸ', 'exit', 'quit', 'q', 'bye']:
                print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere! Ä°yi gÃ¼nler!")
                break
            
            # BoÅŸ input kontrolÃ¼
            if not user_input:
                print("âš ï¸ LÃ¼tfen bir soru yazÄ±n.")
                continue
            
            # Cevap al
            response, sources = chatbot.query(user_input)
            print(f"\nğŸ¤– Asistan:\n{response}")
            
            # KaynaklarÄ± gÃ¶ster
            chatbot.show_sources(sources)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere! Ä°yi gÃ¼nler!")
            break
        except Exception as e:
            print(f"\nâŒ Bir hata oluÅŸtu: {e}")
            import traceback
            traceback.print_exc()
            print("LÃ¼tfen tekrar deneyin.")
    
    # KonuÅŸma Ã¶zeti
    if chatbot.conversation_history:
        print("\n" + "=" * 60)
        print(f"ğŸ“Š Toplam {len(chatbot.conversation_history)} soru cevaplandÄ±.")
        print("=" * 60)

if __name__ == "__main__":
    main()