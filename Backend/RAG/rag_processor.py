# rag_processor_improved.py - AKILLI PDF YÃ–NETÄ°MÄ° (TOKEN BAZLI)
import os
import sys
import warnings
from pathlib import Path
from typing import List, Set, Dict, Optional

# LangChain imports
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

# Tokenizer iÃ§in
try:
    from transformers import AutoTokenizer
    TOKENIZER_AVAILABLE = True
except ImportError:
    TOKENIZER_AVAILABLE = False
    print("âš ï¸ Transformers kÃ¼tÃ¼phanesi kullanÄ±lamÄ±yor")

# PDF Fallback yÃ¼kleyiciler
try:
    from langchain_community.document_loaders import (
        PyPDFLoader, 
        TextLoader,
        UnstructuredWordDocumentLoader
    )
    FALLBACK_LOADERS_AVAILABLE = True
except ImportError:
    FALLBACK_LOADERS_AVAILABLE = False
    print("âš ï¸ LangChain document loaders kullanÄ±lamÄ±yor")

# PyMuPDF
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("âš ï¸ PyMuPDF kullanÄ±lamÄ±yor")

# Unstructured
try:
    from unstructured.partition.auto import partition
    UNSTRUCTURED_AVAILABLE = True
except ImportError:
    UNSTRUCTURED_AVAILABLE = False
    print("âš ï¸ unstructured.io kullanÄ±lamÄ±yor")

# Chroma import
try:
    from langchain_chroma import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    try:
        from langchain_community.vectorstores import Chroma
        CHROMA_AVAILABLE = True
    except ImportError:
        CHROMA_AVAILABLE = False
        print("âŒ ChromaDB bulunamadÄ±!")

warnings.filterwarnings('ignore')

class RAGProcessor:
    def __init__(self, pdfs_path="PDFs", vector_store_path="vector_store", model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2"):
        self.pdfs_path = pdfs_path
        self.vector_store_path = vector_store_path
        self.model_name = model_name
        
        if not CHROMA_AVAILABLE:
            raise ImportError("ChromaDB kÃ¼tÃ¼phanesi yÃ¼klenemedi!")
            
        print("ğŸ”§ MULTILINGUAL Embeddings modeli yÃ¼kleniyor...")  # ğŸ¯ MODEL Ä°SMÄ°
        self.embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… MULTILINGUAL Embeddings hazÄ±r!")  # ğŸ¯ MODEL Ä°SMÄ°
        
        # Tokenizer'Ä± yÃ¼kle (token bazlÄ± bÃ¶lme iÃ§in)
        self.tokenizer = None
        if TOKENIZER_AVAILABLE:
            try:
                print("ğŸ”§ MULTILINGUAL Tokenizer yÃ¼kleniyor...")  # ğŸ¯ MODEL Ä°SMÄ°
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # ğŸ¯ TOKEN LÄ°MÄ°T BÄ°LGÄ°SÄ° EKLE
                model_max_length = self.tokenizer.model_max_length
                print(f"ğŸ‰ MULTILINGUAL Token Limit: {model_max_length}")
                
                print("âœ… Tokenizer hazÄ±r")
            except Exception as e:
                print(f"âš ï¸ Tokenizer yÃ¼klenemedi: {e}")
                self.tokenizer = None
        
        self.vector_store = None
        
        # BaÅŸlangÄ±Ã§ta vektÃ¶r veritabanÄ±nÄ± yÃ¼kle
        self._try_load_vector_store()
    
    def _create_token_text_splitter(self):
        """MULTILINGUAL iÃ§in token bazlÄ± text splitter"""
        if self.tokenizer and TOKENIZER_AVAILABLE:
            try:
                # Modelin token limitine gÃ¶re gÃ¼venli chunk size
                return TokenTextSplitter(
                    chunk_size=1500,
                    chunk_overlap=150
                )
            except Exception as e:
                print(f"âš ï¸ Token splitter oluÅŸturulamadÄ±: {e}")
        
        # Fallback: karakter bazlÄ± ama daha kÃ¼Ã§Ã¼k chunk'lar
        print("âš ï¸ Tokenizer yok, karakter bazlÄ± bÃ¶lme kullanÄ±lÄ±yor")
        return RecursiveCharacterTextSplitter(
            chunk_size=3000,  # Daha kÃ¼Ã§Ã¼k - token sÄ±nÄ±rÄ±na uymasÄ± iÃ§in
            chunk_overlap=300
        )
    
    def _count_tokens(self, text: str) -> int:
        """Metnin token sayÄ±sÄ±nÄ± hesapla"""
        if self.tokenizer and text:
            return len(self.tokenizer.encode(text))
        return len(text) // 4  # Tahmini: 1 token â‰ˆ 4 karakter
    
    def _try_load_vector_store(self):
        """VektÃ¶r veritabanÄ±nÄ± yÃ¼klemeyi dene"""
        try:
            if not os.path.exists(self.vector_store_path):
                print(f"âš ï¸ VektÃ¶r klasÃ¶rÃ¼ bulunamadÄ±: {self.vector_store_path}")
                return False
            
            sqlite_file = os.path.join(self.vector_store_path, "chroma.sqlite3")
            if not os.path.exists(sqlite_file):
                print(f"âš ï¸ chroma.sqlite3 bulunamadÄ±: {sqlite_file}")
                return False
            
            print(f"ğŸ“‚ VektÃ¶r veritabanÄ± yÃ¼kleniyor: {self.vector_store_path}")
            
            self.vector_store = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings
            )
            
            test_results = self.vector_store.similarity_search("test", k=1)
            
            if test_results:
                print(f"âœ… VektÃ¶r veritabanÄ± baÅŸarÄ±yla yÃ¼klendi!")
                return True
            else:
                print("âš ï¸ VektÃ¶r veritabanÄ± boÅŸ gÃ¶rÃ¼nÃ¼yor")
                return False
                
        except Exception as e:
            print(f"âš ï¸ VektÃ¶r veritabanÄ± yÃ¼kleme hatasÄ±: {e}")
            self.vector_store = None
            return False
    
    def _get_files_in_vector_store(self) -> Set[str]:
        """
        VektÃ¶r store'daki tÃ¼m dosyalarÄ±n tam yollarÄ±nÄ± Ã§Ä±kar
        """
        if self.vector_store is None:
            print("âš ï¸ VektÃ¶r store yÃ¼klÃ¼ deÄŸil")
            return set()
        
        try:
            print("ğŸ” VektÃ¶r store'daki dosyalar sorgulanÄ±yor...")
            
            collection = self.vector_store._collection
            all_data = collection.get(include=['metadatas'])
            
            files_in_store = set()
            if all_data and 'metadatas' in all_data:
                for metadata in all_data['metadatas']:
                    if metadata and 'source' in metadata:
                        source_path = Path(metadata['source']).resolve()
                        files_in_store.add(str(source_path))
            
            print(f"âœ… VektÃ¶r store'da {len(files_in_store)} dosya bulundu")
            return files_in_store
            
        except Exception as e:
            print(f"âŒ VektÃ¶r store sorgu hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return set()
    
    def _delete_documents_by_source(self, file_path: str):
        """
        Belirli bir kaynak dosyaya ait tÃ¼m chunk'larÄ± vektÃ¶r store'dan sil
        """
        if self.vector_store is None:
            print("âš ï¸ VektÃ¶r store yÃ¼klÃ¼ deÄŸil")
            return False
        
        try:
            print(f"ğŸ—‘ï¸  Siliniyor: {Path(file_path).name}")
            
            collection = self.vector_store._collection
            
            all_data = collection.get(include=['metadatas'])
            ids_to_delete = []
            
            if all_data and 'ids' in all_data and 'metadatas' in all_data:
                for idx, metadata in enumerate(all_data['metadatas']):
                    if metadata and 'source' in metadata:
                        meta_source = str(Path(metadata['source']).resolve())
                        target_source = str(Path(file_path).resolve())
                        
                        if meta_source == target_source:
                            ids_to_delete.append(all_data['ids'][idx])
            
            if ids_to_delete:
                collection.delete(ids=ids_to_delete)
                print(f"   âœ… {len(ids_to_delete)} chunk silindi")
                return True
            else:
                print(f"   âš ï¸ Silinecek chunk bulunamadÄ±")
                return False
                
        except Exception as e:
            print(f"   âŒ Silme hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_all_document_files(self) -> List[Path]:
        """PDFs klasÃ¶rÃ¼ ve tÃ¼m alt klasÃ¶rlerindeki desteklenen dosyalarÄ± bul"""
        pdfs_path = Path(self.pdfs_path)
        
        if not pdfs_path.exists():
            print(f"âŒ PDFs klasÃ¶rÃ¼ bulunamadÄ±: {self.pdfs_path}")
            return []
        
        supported_extensions = {
            '.pdf', '.doc', '.docx', '.txt', 
            '.rtf', '.odt', '.pptx', '.ppt'
        }
        
        document_files = []
        for ext in supported_extensions:
            files = list(pdfs_path.rglob(f"*{ext}"))
            document_files.extend(files)
        
        document_files = [f.resolve() for f in set(document_files)]
        document_files.sort()
        
        print(f"ğŸ” Tarama tamamlandÄ±. {len(document_files)} dosya bulundu.")
        return document_files
    
    def _load_pdf_with_pymupdf(self, file_path: Path) -> List[Document]:
        """PyMuPDF ile PDF yÃ¼kleme"""
        if not PYMUPDF_AVAILABLE:
            return []
            
        try:
            print(f"   ğŸ“„ PyMuPDF ile yÃ¼kleniyor: {file_path.name}")
            
            doc = fitz.open(file_path)
            documents = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text.strip():
                    document = Document(
                        page_content=text,
                        metadata={
                            "source": str(file_path),
                            "file_name": file_path.name,
                            "file_type": ".pdf",
                            "page": page_num + 1,
                            "total_pages": len(doc),
                            "loader_type": "pymupdf",
                            "token_count": self._count_tokens(text)  # Token sayÄ±sÄ±nÄ± ekle
                        }
                    )
                    documents.append(document)
            
            doc.close()
            
            if documents:
                total_tokens = sum(doc.metadata.get('token_count', 0) for doc in documents)
                print(f"   âœ… {len(documents)} sayfa yÃ¼klendi (PyMuPDF) - Toplam ~{total_tokens} token")
            else:
                print(f"   âš ï¸ PDF aÃ§Ä±ldÄ± ama metin Ã§Ä±karÄ±lamadÄ±")
            
            return documents
            
        except Exception as e:
            print(f"   âš ï¸ PyMuPDF yÃ¼kleme hatasÄ±: {e}")
            return []
    
    def _load_document_with_langchain(self, file_path: Path) -> List[Document]:
        """LangChain yÃ¼kleyicileri ile belge yÃ¼kleme"""
        if not FALLBACK_LOADERS_AVAILABLE:
            return []
            
        try:
            file_ext = file_path.suffix.lower()
            
            if file_ext == '.pdf':
                print(f"   ğŸ“„ LangChain PDF Loader: {file_path.name}")
                loader = PyPDFLoader(str(file_path))
            elif file_ext in ['.doc', '.docx']:
                print(f"   ğŸ“ Word Loader: {file_path.name}")
                loader = UnstructuredWordDocumentLoader(str(file_path))
            elif file_ext == '.txt':
                print(f"   ğŸ“ Text Loader: {file_path.name}")
                loader = TextLoader(str(file_path), encoding='utf-8')
            else:
                print(f"   âš ï¸ Desteklenmeyen dosya tÃ¼rÃ¼: {file_ext}")
                return []
            
            documents = loader.load()
            
            non_empty_docs = []
            for doc in documents:
                doc.metadata.update({
                    "source": str(file_path),
                    "file_name": file_path.name,
                    "file_type": file_ext,
                    "loader_type": "langchain",
                    "token_count": self._count_tokens(doc.page_content)  # Token sayÄ±sÄ±nÄ± ekle
                })
                
                if doc.page_content and doc.page_content.strip():
                    non_empty_docs.append(doc)
            
            if non_empty_docs:
                total_tokens = sum(doc.metadata.get('token_count', 0) for doc in non_empty_docs)
                print(f"   âœ… {len(non_empty_docs)} sayfa yÃ¼klendi (LangChain) - Toplam ~{total_tokens} token")
            else:
                print(f"   âš ï¸ Dosya yÃ¼klendi ama iÃ§erik boÅŸ")
            
            return non_empty_docs
            
        except Exception as e:
            print(f"   âš ï¸ LangChain yÃ¼kleme hatasÄ±: {e}")
            return []
    
    def _load_document_with_unstructured(self, file_path: Path) -> List[Document]:
        """unstructured.io ile geliÅŸmiÅŸ belge yÃ¼kleme"""
        if not UNSTRUCTURED_AVAILABLE:
            return []
            
        try:
            print(f"   ğŸ§  Unstructured.io ile deneniyor: {file_path.name}")
            
            elements = partition(
                filename=str(file_path),
                strategy="fast",
                pdf_infer_table_structure=False,
                languages=["eng"],
            )
            
            documents = []
            for i, element in enumerate(elements):
                content = element.text.strip()
                if content:
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": str(file_path),
                            "file_name": file_path.name,
                            "file_type": file_path.suffix,
                            "element_type": type(element).__name__,
                            "element_index": i,
                            "loader_type": "unstructured",
                            "token_count": self._count_tokens(content)  # Token sayÄ±sÄ±nÄ± ekle
                        }
                    )
                    documents.append(doc)
            
            if documents:
                total_tokens = sum(doc.metadata.get('token_count', 0) for doc in documents)
                print(f"   âœ… {len(documents)} element Ã§Ä±karÄ±ldÄ± (Unstructured) - Toplam ~{total_tokens} token")
            return documents
            
        except Exception as e:
            print(f"   âš ï¸ Unstructured.io hatasÄ±: {e}")
            return []
    
    def _load_single_document(self, file_path: Path) -> List[Document]:
        """Tek bir belgeyi yÃ¼kle"""
        print(f"ğŸ“– YÃ¼kleniyor: {file_path.name}")
        
        file_ext = file_path.suffix.lower()
        
        # PDF iÃ§in Ã¶nce PyMuPDF dene
        if file_ext == '.pdf' and PYMUPDF_AVAILABLE:
            documents = self._load_pdf_with_pymupdf(file_path)
            if documents:
                return documents
            print(f"   âš ï¸ PyMuPDF baÅŸarÄ±sÄ±z, alternatif yÃ¶ntem deneniyor...")
        
        # LangChain yÃ¼kleyicileri
        if FALLBACK_LOADERS_AVAILABLE:
            documents = self._load_document_with_langchain(file_path)
            if documents:
                return documents
            print(f"   âš ï¸ LangChain baÅŸarÄ±sÄ±z, son yÃ¶ntem deneniyor...")
        
        # Son Ã§are: Unstructured.io
        if UNSTRUCTURED_AVAILABLE:
            documents = self._load_document_with_unstructured(file_path)
            if documents:
                return documents
        
        print(f"   âŒ HiÃ§bir yÃ¶ntemle yÃ¼klenemedi: {file_path.name}")
        return []
    
    def load_and_process_documents(self, force_reprocess=False):
        """
        TÃ¼m belgeleri akÄ±llÄ± ÅŸekilde yÃ¼kle ve iÅŸle (TOKEN BAZLI)
        """
        if not CHROMA_AVAILABLE:
            print("âŒ Chroma kullanÄ±lamadÄ±ÄŸÄ± iÃ§in belge iÅŸlenemiyor")
            return False
        
        print("\n" + "="*70)
        print("ğŸš€ MULTILINGUAL AKILLI PDF YÃ–NETÄ°MÄ° BAÅLATILIYOR (TOKEN BAZLI)")
        print("="*70)
        
        # 1. PDFs klasÃ¶rÃ¼ndeki mevcut dosyalarÄ± bul
        current_files = self._get_all_document_files()
        if not current_files:
            print("âŒ Ä°ÅŸlenecek dosya bulunamadÄ±!")
            return False
        
        current_files_set = {str(f) for f in current_files}
        print(f"ğŸ“‚ PDFs klasÃ¶rÃ¼nde {len(current_files_set)} dosya bulundu")
        
        # 2. VektÃ¶r store'daki dosyalarÄ± bul
        files_in_store = set()
        if self.vector_store is not None and not force_reprocess:
            files_in_store = self._get_files_in_vector_store()
        else:
            if force_reprocess:
                print("âš ï¸ FORCE_REPROCESS aktif - TÃ¼m dosyalar yeniden iÅŸlenecek")
            else:
                print("âš ï¸ VektÃ¶r store bulunamadÄ± - TÃ¼m dosyalar iÅŸlenecek")
        
        # 3. FarklarÄ± hesapla
        new_files = current_files_set - files_in_store
        deleted_files = files_in_store - current_files_set
        existing_files = current_files_set & files_in_store
        
        print("\n" + "="*70)
        print("ğŸ“Š DURUM ANALÄ°ZÄ°")
        print("="*70)
        print(f"âœ… Zaten iÅŸlenmiÅŸ: {len(existing_files)} dosya")
        print(f"ğŸ†• Yeni eklenen: {len(new_files)} dosya")
        print(f"ğŸ—‘ï¸  Silinen: {len(deleted_files)} dosya")
        print("="*70)
        
        # 4. Silinen dosyalarÄ± vektÃ¶r store'dan temizle
        if deleted_files:
            print(f"\nğŸ—‘ï¸  Silinen {len(deleted_files)} dosya vektÃ¶r store'dan kaldÄ±rÄ±lÄ±yor...")
            deleted_count = 0
            for deleted_file in deleted_files:
                if self._delete_documents_by_source(deleted_file):
                    deleted_count += 1
            print(f"âœ… {deleted_count} dosya baÅŸarÄ±yla temizlendi")
        
        # 5. Yeni dosyalarÄ± iÅŸle
        if not new_files:
            print("\nâœ… TÃ¼m dosyalar gÃ¼ncel! Ä°ÅŸlenecek yeni dosya yok.")
            return True
        
        print(f"\nğŸ†• {len(new_files)} yeni dosya iÅŸlenecek:")
        new_files_list = [Path(f) for f in sorted(new_files)]
        for i, file_path in enumerate(new_files_list, 1):
            print(f"  {i}. {file_path.name}")
        
        # 6. Yeni dosyalarÄ± yÃ¼kle
        all_documents = []
        successful_files = 0
        failed_files = 0
        empty_content_files = []
        
        print("\nğŸ“– Dosyalar yÃ¼kleniyor...")
        for file_path in new_files_list:
            documents = self._load_single_document(file_path)
            if documents:
                all_documents.extend(documents)
                successful_files += 1
            else:
                failed_files += 1
                empty_content_files.append(file_path.name)
        
        print(f"\nğŸ“Š YÃ¼kleme Ã–zeti:")
        print(f"  âœ… BaÅŸarÄ±lÄ±: {successful_files} dosya")
        print(f"  âŒ BaÅŸarÄ±sÄ±z: {failed_files} dosya")
        print(f"  ğŸ“„ Toplam: {len(all_documents)} dokÃ¼man elementi")
        
        if empty_content_files:
            print(f"\nâš ï¸ Ä°Ã§erik Ã‡Ä±karÄ±lamayan Dosyalar ({len(empty_content_files)}):")
            for file_name in empty_content_files[:10]:
                print(f"   - {file_name}")
            if len(empty_content_files) > 10:
                print(f"   ... ve {len(empty_content_files) - 10} dosya daha")
            print("\nğŸ’¡ Bu dosyalar muhtemelen taranmÄ±ÅŸ gÃ¶rÃ¼ntÃ¼ (OCR gerekli)")
        
        if not all_documents:
            print("âš ï¸ Yeni yÃ¼klenecek dokÃ¼man yok")
            return True
        
        # 7. TOKEN BAZLI METÄ°N BÃ–LME
        print("\nâœ‚ï¸  TOKEN BAZLI metin bÃ¶lme yapÄ±lÄ±yor...")
        text_splitter = self._create_token_text_splitter()
        chunks = text_splitter.split_documents(all_documents)
        
        # Token istatistiklerini hesapla
        total_tokens = sum(self._count_tokens(chunk.page_content) for chunk in chunks)
        avg_tokens = total_tokens // len(chunks) if chunks else 0
        
        print(f"âœ‚ï¸  {len(chunks)} metin parÃ§asÄ± oluÅŸturuldu")
        print(f"ğŸ“Š Token istatistikleri: Toplam ~{total_tokens} token, Ortalama ~{avg_tokens} token/parÃ§a")
        
        # Chunk boyutu analizi
        chunk_sizes = [self._count_tokens(chunk.page_content) for chunk in chunks]
        max_tokens = max(chunk_sizes) if chunk_sizes else 0
        min_tokens = min(chunk_sizes) if chunk_sizes else 0
        
        print(f"ğŸ“ Chunk boyutlarÄ±: Min {min_tokens}, Maks {max_tokens} token")
        
        if max_tokens > 1900:
            print("âš ï¸ UYARI: BazÄ± chunk'lar 1900 token sÄ±nÄ±rÄ±na yaklaÅŸÄ±yor!")
        elif max_tokens > 1500:
            print("âœ… Chunk boyutlarÄ± optimum aralÄ±kta")
        
        # BoÅŸ chunk kontrolÃ¼
        if len(chunks) == 0:
            print("âš ï¸ UYARI: HiÃ§ metin parÃ§asÄ± oluÅŸturulamadÄ±!")
            print("âœ… Silme iÅŸlemi tamamlandÄ± ama yeni ekleme yapÄ±lamadÄ±")
            return True
        
        # 8. VektÃ¶r store'a ekle
        print("ğŸ”§ Yeni dokÃ¼manlar vektÃ¶r veritabanÄ±na ekleniyor...")
        try:
            if self.vector_store is None:
                self.vector_store = Chroma.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    persist_directory=self.vector_store_path
                )
                print("âœ… VektÃ¶r veritabanÄ± oluÅŸturuldu!")
            else:
                self.vector_store.add_documents(chunks)
                print("âœ… Yeni dokÃ¼manlar eklendi!")
            
            print("\n" + "="*70)
            print("ğŸ‰ TOKEN BAZLI Ä°ÅLEM TAMAMLANDI")
            print("="*70)
            return True
            
        except Exception as e:
            print(f"âŒ VektÃ¶r veritabanÄ± iÅŸlemi baÅŸarÄ±sÄ±z: {e}")
            import traceback
            traceback.print_exc()
            return False
        
    def search_similar(self, query, k=3):
        """Benzer dokÃ¼manlarÄ± ara"""
        if not CHROMA_AVAILABLE:
            print("âŒ Chroma kullanÄ±lamÄ±yor!")
            return []
        
        if self.vector_store is None:
            print("ğŸ”„ VektÃ¶r veritabanÄ± yeniden yÃ¼kleniyor...")
            success = self._try_load_vector_store()
            
            if not success:
                print("âŒ VektÃ¶r veritabanÄ± yÃ¼klenemedi. Belgeleri iÅŸlemeniz gerekiyor.")
                return []
        
        try:
            print(f"ğŸ” Arama yapÄ±lÄ±yor: '{query}'")
            results = self.vector_store.similarity_search(query, k=k)
            print(f"âœ… {len(results)} sonuÃ§ bulundu")
            return results
        except Exception as e:
            print(f"âŒ Arama hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_vector_store_stats(self):
        """VektÃ¶r store istatistiklerini gÃ¶ster"""
        if self.vector_store is None:
            print("âš ï¸ VektÃ¶r store yÃ¼klÃ¼ deÄŸil")
            return
        
        try:
            files_in_store = self._get_files_in_vector_store()
            
            print("\n" + "="*70)
            print("ğŸ“Š VEKTÃ–R STORE Ä°STATÄ°STÄ°KLERÄ°")
            print("="*70)
            print(f"Toplam dosya sayÄ±sÄ±: {len(files_in_store)}")
            print("\nDosyalar:")
            for i, file_path in enumerate(sorted(files_in_store), 1):
                print(f"  {i}. {Path(file_path).name}")
            print("="*70)
            
        except Exception as e:
            print(f"âŒ Ä°statistik hatasÄ±: {e}")

def print_system_info():
    """Sistem bilgilerini yazdÄ±r"""
    print("=" * 70)
    print("ğŸ” SISTEM BÄ°LGÄ°LERÄ°")
    print("=" * 70)
    print(f"Python Version: {sys.version}")
    print(f"PyMuPDF Available: {PYMUPDF_AVAILABLE}")
    print(f"LangChain Loaders Available: {FALLBACK_LOADERS_AVAILABLE}")
    print(f"Unstructured Available: {UNSTRUCTURED_AVAILABLE}")
    print(f"Chroma Available: {CHROMA_AVAILABLE}")
    print(f"Tokenizer Available: {TOKENIZER_AVAILABLE}")
    print("=" * 70)
    print()

def main():
    """RAG Processor test fonksiyonu"""
    print_system_info()
    
    print("ğŸ§ª MULTILINGUAL AKILLI RAG PROCESSOR TEST EDÄ°LÄ°YOR... (TOKEN BAZLI)")
    print()
    
    # Processor'Ä± baÅŸlat
    processor = RAGProcessor()

    # ğŸ¯ MODEL BÄ°LGÄ°SÄ°NÄ° GÃ–STER
    print(f"ğŸ”§ KullanÄ±lan Model: {processor.model_name}")
    if processor.tokenizer:
        print(f"ğŸ”§ Token Limit: {processor.tokenizer.model_max_length}")
    print()
    
    # VektÃ¶r store istatistiklerini gÃ¶ster
    if processor.vector_store is not None:
        processor.get_vector_store_stats()
    
    # AkÄ±llÄ± iÅŸleme - sadece yeni dosyalarÄ± iÅŸle
    print("\nğŸ“š AkÄ±llÄ± PDF iÅŸleme baÅŸlatÄ±lÄ±yor...")
    success = processor.load_and_process_documents(force_reprocess=False)
    
    if success:
        print("\nâœ… Ä°ÅŸlem baÅŸarÄ±yla tamamlandÄ±!")
        
        # GÃ¼ncel istatistikleri gÃ¶ster
        processor.get_vector_store_stats()
        
        # Test aramasÄ± yap
        print("\nğŸ” Test aramasÄ± yapÄ±lÄ±yor...")
        results = processor.search_similar("organik tarÄ±m", k=2)
        
        if results:
            print("\nğŸ“„ Ä°lk SonuÃ§:")
            print(f"Kaynak: {results[0].metadata.get('file_name', 'Bilinmiyor')}")
            content_preview = results[0].page_content[:200]
            token_count = processor._count_tokens(results[0].page_content)
            print(f"Token sayÄ±sÄ±: {token_count}")
            print(f"Ä°Ã§erik Ã¶nizleme: {content_preview}...")
    else:
        print("\nâŒ Ä°ÅŸlem baÅŸarÄ±sÄ±z!")

if __name__ == "__main__":
    main()