# rag_processor_improved.py - AKILLI PDF Y√ñNETƒ∞Mƒ∞
import os
import sys
import warnings
from pathlib import Path
from typing import List, Set, Dict, Optional

# LangChain imports
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

# PDF Fallback y√ºkleyiciler
try:
    from langchain_community.document_loaders import (
        PyPDFLoader, 
        TextLoader,
        UnstructuredWordDocumentLoader
    )
    FALLBACK_LOADERS_AVAILABLE = True
except ImportError:
    FALLBACK_LOADERS_AVAILABLE = False
    print("‚ö†Ô∏è LangChain document loaders kullanƒ±lamƒ±yor")

# PyMuPDF
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("‚ö†Ô∏è PyMuPDF kullanƒ±lamƒ±yor")

# Unstructured
try:
    from unstructured.partition.auto import partition
    UNSTRUCTURED_AVAILABLE = True
except ImportError:
    UNSTRUCTURED_AVAILABLE = False
    print("‚ö†Ô∏è unstructured.io kullanƒ±lamƒ±yor")

# Chroma import
try:
    from langchain_chroma import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    try:
        from langchain_community.vectorstores import Chroma
        CHROMA_AVAILABLE = True
    except ImportError:
        CHROMA_AVAILABLE = False
        print("‚ùå ChromaDB bulunamadƒ±!")

warnings.filterwarnings('ignore')

class RAGProcessor:
    def __init__(self, pdfs_path="PDFs", vector_store_path="vector_store"):
        self.pdfs_path = pdfs_path
        self.vector_store_path = vector_store_path
        
        if not CHROMA_AVAILABLE:
            raise ImportError("ChromaDB k√ºt√ºphanesi y√ºklenemedi!")
            
        print("üîß Embeddings modeli y√ºkleniyor...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        print("‚úÖ Embeddings hazƒ±r")
        
        self.vector_store = None
        
        # Ba≈ülangƒ±√ßta vekt√∂r veritabanƒ±nƒ± y√ºkle
        self._try_load_vector_store()
        
    def _try_load_vector_store(self):
        """Vekt√∂r veritabanƒ±nƒ± y√ºklemeyi dene"""
        try:
            # Vekt√∂r store klas√∂r√ºn√º kontrol et
            if not os.path.exists(self.vector_store_path):
                print(f"‚ö†Ô∏è Vekt√∂r klas√∂r√º bulunamadƒ±: {self.vector_store_path}")
                return False
            
            # chroma.sqlite3 dosyasƒ±nƒ± kontrol et
            sqlite_file = os.path.join(self.vector_store_path, "chroma.sqlite3")
            if not os.path.exists(sqlite_file):
                print(f"‚ö†Ô∏è chroma.sqlite3 bulunamadƒ±: {sqlite_file}")
                return False
            
            print(f"üìÇ Vekt√∂r veritabanƒ± y√ºkleniyor: {self.vector_store_path}")
            
            # Chroma'yƒ± y√ºkle
            self.vector_store = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings
            )
            
            # Test sorgusu yaparak kontrol et
            test_results = self.vector_store.similarity_search("test", k=1)
            
            if test_results:
                print(f"‚úÖ Vekt√∂r veritabanƒ± ba≈üarƒ±yla y√ºklendi!")
                return True
            else:
                print("‚ö†Ô∏è Vekt√∂r veritabanƒ± bo≈ü g√∂r√ºn√ºyor")
                return False
                
        except Exception as e:
            print(f"‚ö†Ô∏è Vekt√∂r veritabanƒ± y√ºkleme hatasƒ±: {e}")
            self.vector_store = None
            return False
    
    def _get_files_in_vector_store(self) -> Set[str]:
        """
        Vekt√∂r store'daki t√ºm dosyalarƒ±n tam yollarƒ±nƒ± √ßƒ±kar
        Returns: Set of absolute file paths
        """
        if self.vector_store is None:
            print("‚ö†Ô∏è Vekt√∂r store y√ºkl√º deƒüil")
            return set()
        
        try:
            print("üîç Vekt√∂r store'daki dosyalar sorgulanƒ±yor...")
            
            # Chroma'dan t√ºm metadata'larƒ± al
            # Bu, vekt√∂r store'daki t√ºm chunk'larƒ± d√∂nd√ºr√ºr
            collection = self.vector_store._collection
            all_data = collection.get(include=['metadatas'])
            
            # Metadata'lardan unique file path'leri √ßƒ±kar
            files_in_store = set()
            if all_data and 'metadatas' in all_data:
                for metadata in all_data['metadatas']:
                    if metadata and 'source' in metadata:
                        # Absolute path'e √ßevir
                        source_path = Path(metadata['source']).resolve()
                        files_in_store.add(str(source_path))
            
            print(f"‚úÖ Vekt√∂r store'da {len(files_in_store)} dosya bulundu")
            return files_in_store
            
        except Exception as e:
            print(f"‚ùå Vekt√∂r store sorgu hatasƒ±: {e}")
            import traceback
            traceback.print_exc()
            return set()
    
    def _delete_documents_by_source(self, file_path: str):
        """
        Belirli bir kaynak dosyaya ait t√ºm chunk'larƒ± vekt√∂r store'dan sil
        """
        if self.vector_store is None:
            print("‚ö†Ô∏è Vekt√∂r store y√ºkl√º deƒüil")
            return False
        
        try:
            print(f"üóëÔ∏è  Siliniyor: {Path(file_path).name}")
            
            collection = self.vector_store._collection
            
            # Bu dosyaya ait t√ºm ID'leri bul
            all_data = collection.get(include=['metadatas'])
            ids_to_delete = []
            
            if all_data and 'ids' in all_data and 'metadatas' in all_data:
                for idx, metadata in enumerate(all_data['metadatas']):
                    if metadata and 'source' in metadata:
                        # Path'leri normalize ederek kar≈üƒ±la≈ütƒ±r
                        meta_source = str(Path(metadata['source']).resolve())
                        target_source = str(Path(file_path).resolve())
                        
                        if meta_source == target_source:
                            ids_to_delete.append(all_data['ids'][idx])
            
            if ids_to_delete:
                collection.delete(ids=ids_to_delete)
                print(f"   ‚úÖ {len(ids_to_delete)} chunk silindi")
                return True
            else:
                print(f"   ‚ö†Ô∏è Silinecek chunk bulunamadƒ±")
                return False
                
        except Exception as e:
            print(f"   ‚ùå Silme hatasƒ±: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_all_document_files(self) -> List[Path]:
        """PDFs klas√∂r√º ve t√ºm alt klas√∂rlerindeki desteklenen dosyalarƒ± bul"""
        pdfs_path = Path(self.pdfs_path)
        
        if not pdfs_path.exists():
            print(f"‚ùå PDFs klas√∂r√º bulunamadƒ±: {self.pdfs_path}")
            return []
        
        # Desteklenen dosya uzantƒ±larƒ±
        supported_extensions = {
            '.pdf', '.doc', '.docx', '.txt', 
            '.rtf', '.odt', '.pptx', '.ppt'
        }
        
        # T√ºm alt klas√∂rleri tarayarak dosyalarƒ± bul
        document_files = []
        for ext in supported_extensions:
            files = list(pdfs_path.rglob(f"*{ext}"))
            document_files.extend(files)
        
        # Benzersiz dosya listesi - absolute path'e √ßevir
        document_files = [f.resolve() for f in set(document_files)]
        document_files.sort()
        
        print(f"üîç Tarama tamamlandƒ±. {len(document_files)} dosya bulundu.")
        return document_files
    
    def _load_pdf_with_pymupdf(self, file_path: Path) -> List[Document]:
        """PyMuPDF ile PDF y√ºkleme - EN G√úVENƒ∞Lƒ∞R Y√ñNTEM"""
        if not PYMUPDF_AVAILABLE:
            return []
            
        try:
            print(f"   üìÑ PyMuPDF ile y√ºkleniyor: {file_path.name}")
            
            doc = fitz.open(file_path)
            documents = []
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                # Sadece i√ßeriƒüi olan sayfalarƒ± ekle
                if text.strip():
                    document = Document(
                        page_content=text,
                        metadata={
                            "source": str(file_path),
                            "file_name": file_path.name,
                            "file_type": ".pdf",
                            "page": page_num + 1,
                            "total_pages": len(doc),
                            "loader_type": "pymupdf"
                        }
                    )
                    documents.append(document)
            
            doc.close()
            
            if documents:
                print(f"   ‚úÖ {len(documents)} sayfa y√ºklendi (PyMuPDF)")
            else:
                print(f"   ‚ö†Ô∏è PDF a√ßƒ±ldƒ± ama metin √ßƒ±karƒ±lamadƒ± (taranmƒ±≈ü g√∂r√ºnt√º olabilir)")
            
            return documents
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è PyMuPDF y√ºkleme hatasƒ±: {e}")
            return []
    
    def _load_document_with_langchain(self, file_path: Path) -> List[Document]:
        """LangChain y√ºkleyicileri ile belge y√ºkleme"""
        if not FALLBACK_LOADERS_AVAILABLE:
            return []
            
        try:
            file_ext = file_path.suffix.lower()
            
            if file_ext == '.pdf':
                print(f"   üìÑ LangChain PDF Loader: {file_path.name}")
                loader = PyPDFLoader(str(file_path))
            elif file_ext in ['.doc', '.docx']:
                print(f"   üìù Word Loader: {file_path.name}")
                loader = UnstructuredWordDocumentLoader(str(file_path))
            elif file_ext == '.txt':
                print(f"   üìù Text Loader: {file_path.name}")
                loader = TextLoader(str(file_path), encoding='utf-8')
            else:
                print(f"   ‚ö†Ô∏è Desteklenmeyen dosya t√ºr√º: {file_ext}")
                return []
            
            documents = loader.load()
            
            # Bo≈ü i√ßerik kontrol√º
            non_empty_docs = []
            for doc in documents:
                # Metadata'yi g√ºncelle
                doc.metadata.update({
                    "source": str(file_path),
                    "file_name": file_path.name,
                    "file_type": file_ext,
                    "loader_type": "langchain"
                })
                
                # Sadece i√ßeriƒüi olan dok√ºmanlarƒ± ekle
                if doc.page_content and doc.page_content.strip():
                    non_empty_docs.append(doc)
            
            if non_empty_docs:
                print(f"   ‚úÖ {len(non_empty_docs)} sayfa y√ºklendi (LangChain)")
            else:
                print(f"   ‚ö†Ô∏è Dosya y√ºklendi ama i√ßerik bo≈ü (OCR gerekebilir)")
            
            return non_empty_docs
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è LangChain y√ºkleme hatasƒ±: {e}")
            return []
    
    def _load_document_with_unstructured(self, file_path: Path) -> List[Document]:
        """unstructured.io ile geli≈ümi≈ü belge y√ºkleme - SADECE Dƒ∞ƒûERLERƒ∞ BA≈ûARISIZ OLURSA"""
        if not UNSTRUCTURED_AVAILABLE:
            return []
            
        try:
            print(f"   üß† Unstructured.io ile deneniyor: {file_path.name}")
            
            elements = partition(
                filename=str(file_path),
                strategy="fast",
                pdf_infer_table_structure=False,
                languages=["eng"],
            )
            
            documents = []
            for i, element in enumerate(elements):
                content = element.text.strip()
                if content:
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": str(file_path),
                            "file_name": file_path.name,
                            "file_type": file_path.suffix,
                            "element_type": type(element).__name__,
                            "element_index": i,
                            "loader_type": "unstructured"
                        }
                    )
                    documents.append(doc)
            
            print(f"   ‚úÖ {len(documents)} element √ßƒ±karƒ±ldƒ± (Unstructured)")
            return documents
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Unstructured.io hatasƒ±: {e}")
            return []
    
    def _load_single_document(self, file_path: Path) -> List[Document]:
        """
        Tek bir belgeyi y√ºkle
        √ñNCELIK SIRASI:
        1. PyMuPDF (PDF i√ßin en g√ºvenilir)
        2. LangChain Loaders
        3. Unstructured.io (son √ßare)
        """
        print(f"üìñ Y√ºkleniyor: {file_path.name}")
        
        file_ext = file_path.suffix.lower()
        
        # PDF i√ßin √∂nce PyMuPDF dene
        if file_ext == '.pdf' and PYMUPDF_AVAILABLE:
            documents = self._load_pdf_with_pymupdf(file_path)
            if documents:
                return documents
            print(f"   ‚ö†Ô∏è PyMuPDF ba≈üarƒ±sƒ±z, alternatif y√∂ntem deneniyor...")
        
        # LangChain y√ºkleyicileri
        if FALLBACK_LOADERS_AVAILABLE:
            documents = self._load_document_with_langchain(file_path)
            if documents:
                return documents
            print(f"   ‚ö†Ô∏è LangChain ba≈üarƒ±sƒ±z, son y√∂ntem deneniyor...")
        
        # Son √ßare: Unstructured.io
        if UNSTRUCTURED_AVAILABLE:
            documents = self._load_document_with_unstructured(file_path)
            if documents:
                return documents
        
        print(f"   ‚ùå Hi√ßbir y√∂ntemle y√ºklenemedi: {file_path.name}")
        return []
    
    def load_and_process_documents(self, force_reprocess=False):
        """
        T√ºm belgeleri akƒ±llƒ± ≈üekilde y√ºkle ve i≈üle
        
        Args:
            force_reprocess: True ise t√ºm PDF'leri yeniden i≈üle (varsayƒ±lan: False)
        """
        if not CHROMA_AVAILABLE:
            print("‚ùå Chroma kullanƒ±lamadƒ±ƒüƒ± i√ßin belge i≈ülenemiyor")
            return False
        
        print("\n" + "="*70)
        print("üöÄ AKILLI PDF Y√ñNETƒ∞Mƒ∞ BA≈ûLATILIYOR")
        print("="*70)
        
        # 1. PDFs klas√∂r√ºndeki mevcut dosyalarƒ± bul
        current_files = self._get_all_document_files()
        if not current_files:
            print("‚ùå ƒ∞≈ülenecek dosya bulunamadƒ±!")
            return False
        
        current_files_set = {str(f) for f in current_files}
        print(f"üìÇ PDFs klas√∂r√ºnde {len(current_files_set)} dosya bulundu")
        
        # 2. Vekt√∂r store'daki dosyalarƒ± bul
        files_in_store = set()
        if self.vector_store is not None and not force_reprocess:
            files_in_store = self._get_files_in_vector_store()
        else:
            if force_reprocess:
                print("‚ö†Ô∏è FORCE_REPROCESS aktif - T√ºm dosyalar yeniden i≈ülenecek")
            else:
                print("‚ö†Ô∏è Vekt√∂r store bulunamadƒ± - T√ºm dosyalar i≈ülenecek")
        
        # 3. Farklarƒ± hesapla
        new_files = current_files_set - files_in_store
        deleted_files = files_in_store - current_files_set
        existing_files = current_files_set & files_in_store
        
        print("\n" + "="*70)
        print("üìä DURUM ANALƒ∞Zƒ∞")
        print("="*70)
        print(f"‚úÖ Zaten i≈ülenmi≈ü: {len(existing_files)} dosya")
        print(f"üÜï Yeni eklenen: {len(new_files)} dosya")
        print(f"üóëÔ∏è  Silinen: {len(deleted_files)} dosya")
        print("="*70)
        
        # 4. Silinen dosyalarƒ± vekt√∂r store'dan temizle
        if deleted_files:
            print(f"\nüóëÔ∏è  Silinen {len(deleted_files)} dosya vekt√∂r store'dan kaldƒ±rƒ±lƒ±yor...")
            deleted_count = 0
            for deleted_file in deleted_files:
                if self._delete_documents_by_source(deleted_file):
                    deleted_count += 1
            print(f"‚úÖ {deleted_count} dosya ba≈üarƒ±yla temizlendi")
        
        # 5. Yeni dosyalarƒ± i≈üle
        if not new_files:
            print("\n‚úÖ T√ºm dosyalar g√ºncel! ƒ∞≈ülenecek yeni dosya yok.")
            return True
        
        print(f"\nüÜï {len(new_files)} yeni dosya i≈ülenecek:")
        new_files_list = [Path(f) for f in sorted(new_files)]
        for i, file_path in enumerate(new_files_list, 1):
            print(f"  {i}. {file_path.name}")
        
        # 6. Yeni dosyalarƒ± y√ºkle
        all_documents = []
        successful_files = 0
        failed_files = 0
        empty_content_files = []  # Bo≈ü i√ßerikli dosyalar
        
        print("\nüìñ Dosyalar y√ºkleniyor...")
        for file_path in new_files_list:
            documents = self._load_single_document(file_path)
            if documents:
                all_documents.extend(documents)
                successful_files += 1
            else:
                failed_files += 1
                empty_content_files.append(file_path.name)
        
        print(f"\nüìä Y√ºkleme √ñzeti:")
        print(f"  ‚úÖ Ba≈üarƒ±lƒ±: {successful_files} dosya")
        print(f"  ‚ùå Ba≈üarƒ±sƒ±z: {failed_files} dosya")
        print(f"  üìÑ Toplam: {len(all_documents)} dok√ºman elementi")
        
        if empty_content_files:
            print(f"\n‚ö†Ô∏è ƒ∞√ßerik √áƒ±karƒ±lamayan Dosyalar ({len(empty_content_files)}):")
            for file_name in empty_content_files[:10]:  # ƒ∞lk 10'unu g√∂ster
                print(f"   - {file_name}")
            if len(empty_content_files) > 10:
                print(f"   ... ve {len(empty_content_files) - 10} dosya daha")
            print("\nüí° Bu dosyalar muhtemelen taranmƒ±≈ü g√∂r√ºnt√º (OCR gerekli)")
        
        if not all_documents:
            print("‚ö†Ô∏è Yeni y√ºklenecek dok√ºman yok")
            return True  # Silme i≈ülemi ba≈üarƒ±lƒ± olmu≈ü olabilir
        
        # 7. Metinleri b√∂l
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(all_documents)
        print(f"‚úÇÔ∏è {len(chunks)} metin par√ßasƒ± olu≈üturuldu")
        
        # Bo≈ü chunk kontrol√º
        if len(chunks) == 0:
            print("‚ö†Ô∏è UYARI: Hi√ß metin par√ßasƒ± olu≈üturulamadƒ±!")
            print("   Muhtemel sebepler:")
            print("   - PDF'ler taranmƒ±≈ü g√∂r√ºnt√º (OCR gerekli)")
            print("   - PDF'ler ≈üifreli veya bozuk")
            print("   - Dosyalarda metin i√ßeriƒüi yok")
            print("\n‚úÖ Silme i≈ülemi tamamlandƒ± ama yeni ekleme yapƒ±lamadƒ±")
            return True
        
        # 8. Vekt√∂r store'a ekle
        print("üîß Yeni dok√ºmanlar vekt√∂r veritabanƒ±na ekleniyor...")
        try:
            if self.vector_store is None:
                # ƒ∞lk kez olu≈üturuluyorsa
                self.vector_store = Chroma.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    persist_directory=self.vector_store_path
                )
                print("‚úÖ Vekt√∂r veritabanƒ± olu≈üturuldu!")
            else:
                # Mevcut store'a ekle
                self.vector_store.add_documents(chunks)
                print("‚úÖ Yeni dok√ºmanlar eklendi!")
            
            print("\n" + "="*70)
            print("üéâ ƒ∞≈ûLEM TAMAMLANDI")
            print("="*70)
            return True
            
        except Exception as e:
            print(f"‚ùå Vekt√∂r veritabanƒ± i≈ülemi ba≈üarƒ±sƒ±z: {e}")
            import traceback
            traceback.print_exc()
            return False
        
    def search_similar(self, query, k=3):
        """Benzer dok√ºmanlarƒ± ara"""
        if not CHROMA_AVAILABLE:
            print("‚ùå Chroma kullanƒ±lamƒ±yor!")
            return []
        
        # Vekt√∂r store yoksa y√ºklemeyi dene
        if self.vector_store is None:
            print("üîÑ Vekt√∂r veritabanƒ± yeniden y√ºkleniyor...")
            success = self._try_load_vector_store()
            
            if not success:
                print("‚ùå Vekt√∂r veritabanƒ± y√ºklenemedi. Belgeleri i≈ülemeniz gerekiyor.")
                return []
        
        try:
            print(f"üîç Arama yapƒ±lƒ±yor: '{query}'")
            results = self.vector_store.similarity_search(query, k=k)
            print(f"‚úÖ {len(results)} sonu√ß bulundu")
            return results
        except Exception as e:
            print(f"‚ùå Arama hatasƒ±: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_vector_store_stats(self):
        """Vekt√∂r store istatistiklerini g√∂ster"""
        if self.vector_store is None:
            print("‚ö†Ô∏è Vekt√∂r store y√ºkl√º deƒüil")
            return
        
        try:
            files_in_store = self._get_files_in_vector_store()
            
            print("\n" + "="*70)
            print("üìä VEKT√ñR STORE ƒ∞STATƒ∞STƒ∞KLERƒ∞")
            print("="*70)
            print(f"Toplam dosya sayƒ±sƒ±: {len(files_in_store)}")
            print("\nDosyalar:")
            for i, file_path in enumerate(sorted(files_in_store), 1):
                print(f"  {i}. {Path(file_path).name}")
            print("="*70)
            
        except Exception as e:
            print(f"‚ùå ƒ∞statistik hatasƒ±: {e}")

def print_system_info():
    """Sistem bilgilerini yazdƒ±r"""
    print("=" * 70)
    print("üîç SISTEM Bƒ∞LGƒ∞LERƒ∞")
    print("=" * 70)
    print(f"Python Version: {sys.version}")
    print(f"PyMuPDF Available: {PYMUPDF_AVAILABLE}")
    print(f"LangChain Loaders Available: {FALLBACK_LOADERS_AVAILABLE}")
    print(f"Unstructured Available: {UNSTRUCTURED_AVAILABLE}")
    print(f"Chroma Available: {CHROMA_AVAILABLE}")
    print("=" * 70)
    print()
        
def main():
    """RAG Processor test fonksiyonu"""
    print_system_info()
    
    print("üß™ AKILLI RAG PROCESSOR TEST EDƒ∞Lƒ∞YOR...")
    print()
    
    # Processor'ƒ± ba≈ülat
    processor = RAGProcessor()
    
    # Vekt√∂r store istatistiklerini g√∂ster
    if processor.vector_store is not None:
        processor.get_vector_store_stats()
    
    # Akƒ±llƒ± i≈üleme - sadece yeni dosyalarƒ± i≈üle
    print("\nüìö Akƒ±llƒ± PDF i≈üleme ba≈ülatƒ±lƒ±yor...")
    success = processor.load_and_process_documents(force_reprocess=False)
    
    if success:
        print("\n‚úÖ ƒ∞≈ülem ba≈üarƒ±yla tamamlandƒ±!")
        
        # G√ºncel istatistikleri g√∂ster
        processor.get_vector_store_stats()
        
        # Test aramasƒ± yap
        print("\nüîç Test aramasƒ± yapƒ±lƒ±yor...")
        results = processor.search_similar("organik tarƒ±m", k=2)
        
        if results:
            print("\nüìÑ ƒ∞lk Sonu√ß:")
            print(f"Kaynak: {results[0].metadata.get('file_name', 'Bilinmiyor')}")
            print(f"ƒ∞√ßerik √∂nizleme: {results[0].page_content[:200]}...")
    else:
        print("\n‚ùå ƒ∞≈ülem ba≈üarƒ±sƒ±z!")

if __name__ == "__main__":
    main()