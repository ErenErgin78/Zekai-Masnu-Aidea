# ============================================================
# ğŸ¤– Crop & Soil â€” Classification Pipeline
# ============================================================
# Ã–n hazÄ±rlÄ±k pipeline'Ä±ndan sonra Ã§alÄ±ÅŸtÄ±rÄ±lacak modelleme kÄ±smÄ±
# - Ã‡oklu sÄ±nÄ±flandÄ±rma modelleri
# - Cross-validation + hiperparametre optimizasyonu
# - Model deÄŸerlendirme ve karÅŸÄ±laÅŸtÄ±rma
# - En iyi modeli kaydetme
# ============================================================

# ---------- IMPORTS ----------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import sys
import os

# Scikit-learn modelleri
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    accuracy_score, f1_score, precision_score, recall_score
)

# ---------- DATA YÃœKLEME ----------
def load_cleaned_data():
    """TemizlenmiÅŸ veriyi ve objeleri yÃ¼kler"""
    try:
        # Data_Cleaning modÃ¼lÃ¼nden verileri al
        from Data_Cleaning import load_and_clean_data
        df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = load_and_clean_data()
        
        print("âœ… TemizlenmiÅŸ veri baÅŸarÄ±yla yÃ¼klendi")
        return df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler
    
    except ImportError:
        print("âŒ Data_Cleaning modÃ¼lÃ¼ bulunamadÄ±, dosyayÄ± doÄŸrudan yÃ¼klemeye Ã§alÄ±ÅŸÄ±yorum...")
        
        # Alternatif: TemizlenmiÅŸ CSV'den yÃ¼kle
        CLEAN_PATH = r"C:\Users\HUSOCAN\Desktop\Projelerim\Zekai-Masnu-Aidea\Backend\API\MachineLearning\Data\Crop_Soil_Final.csv"
        SCALER_PATH = r"C:\Users\HUSOCAN\Desktop\Projelerim\Zekai-Masnu-Aidea\Backend\API\MachineLearning\Data\scaler.pkl"
        
        if os.path.exists(CLEAN_PATH):
            df = pd.read_csv(CLEAN_PATH, encoding="utf-8-sig", sep=";")
            print(f"âœ… TemizlenmiÅŸ veri dosyadan yÃ¼klendi: {df.shape}")
            
            # Train-test split
            TARGET_COL = "Crop_Type"
            X = df.drop(columns=[TARGET_COL])
            y = df[TARGET_COL]
            
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.20, random_state=42, stratify=y
            )
            
            # Scaler yÃ¼kle
            if os.path.exists(SCALER_PATH):
                scaler = joblib.load(SCALER_PATH)
                # Scaling uygula
                num_cols = X_train.select_dtypes(include=[np.number]).columns
                X_train_scaled = scaler.transform(X_train[num_cols])
                X_test_scaled = scaler.transform(X_test[num_cols])
            else:
                print("âŒ Scaler bulunamadÄ±, scaling uygulanamÄ±yor")
                X_train_scaled, X_test_scaled = None, None
            
            return df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler
        else:
            raise FileNotFoundError("TemizlenmiÅŸ veri dosyasÄ± bulunamadÄ±. Ã–nce Data_Cleaning.py'yi Ã§alÄ±ÅŸtÄ±rÄ±n.")

# ---------- 1) MODEL SETUP ----------
def setup_models():
    """Modelleri ve hiperparametre grid'lerini hazÄ±rla"""
    
    # Temel modeller
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, multi_class='ovr'),
        'Random Forest': RandomForestClassifier(random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'SVM': SVC(random_state=42, probability=True),
        'K-Nearest Neighbors': KNeighborsClassifier()
    }

    # Hiperparametre grid'leri
    param_grids = {
        'Logistic Regression': {
            'C': [0.1, 1, 10],
            'solver': ['liblinear', 'saga']
        },
        'Random Forest': {
            'n_estimators': [100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5]
        },
        'Gradient Boosting': {
            'n_estimators': [100, 200],
            'learning_rate': [0.05, 0.1],
            'max_depth': [3, 5]
        },
        'SVM': {
            'C': [0.1, 1, 10],
            'kernel': ['linear', 'rbf']
        },
        'K-Nearest Neighbors': {
            'n_neighbors': [3, 5, 7],
            'weights': ['uniform', 'distance']
        }
    }
    
    return models, param_grids

# ---------- 2) MODEL EÄÄ°TÄ°MÄ° VE OPTÄ°MÄ°ZASYONU ----------
def train_and_evaluate_models(X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, use_scaled=True):
    """TÃ¼m modelleri eÄŸitir, optimize eder ve deÄŸerlendirir"""
    
    models, param_grids = setup_models()
    results = {}
    best_models = {}
    
    # Scaling gerektiren modeller
    scaling_models = ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']
    
    for model_name, model in models.items():
        print(f"\nğŸ¯ {model_name} eÄŸitiliyor...")
        
        # Veri seÃ§imi
        if model_name in scaling_models and use_scaled and X_train_scaled is not None:
            X_tr = X_train_scaled
            X_te = X_test_scaled
            print(f"   Scaled data kullanÄ±lÄ±yor")
        else:
            X_tr = X_train
            X_te = X_test
            print(f"   Normal data kullanÄ±lÄ±yor")
        
        # Cross-validation
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(model, X_tr, y_train, cv=cv, scoring='accuracy')
        
        print(f"   CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # Hiperparametre optimizasyonu
        if model_name in param_grids:
            print(f"   Hiperparametre optimizasyonu baÅŸlatÄ±lÄ±yor...")
            grid_search = GridSearchCV(
                model, param_grids[model_name], 
                cv=5, scoring='accuracy', n_jobs=-1, verbose=0
            )
            grid_search.fit(X_tr, y_train)
            
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            best_score = grid_search.best_score_
            
            print(f"   Best params: {best_params}")
            print(f"   Best CV Score: {best_score:.4f}")
        else:
            best_model = model
            best_model.fit(X_tr, y_train)
            best_score = cv_scores.mean()
            best_params = "No optimization"
        
        # Test seti deÄŸerlendirmesi
        y_pred = best_model.predict(X_te)
        test_accuracy = accuracy_score(y_test, y_pred)
        test_f1 = f1_score(y_test, y_pred, average='weighted')
        test_precision = precision_score(y_test, y_pred, average='weighted')
        test_recall = recall_score(y_test, y_pred, average='weighted')
        
        print(f"   Test Accuracy: {test_accuracy:.4f}")
        print(f"   Test F1-Score: {test_f1:.4f}")
        
        # SonuÃ§larÄ± kaydet
        results[model_name] = {
            'model': best_model,
            'cv_score': best_score,
            'test_accuracy': test_accuracy,
            'test_f1': test_f1,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'best_params': best_params
        }
        
        best_models[model_name] = best_model
    
    return results, best_models

# ---------- 3) MODEL KARÅILAÅTIRMA ----------
def compare_models(results):
    """Modelleri gÃ¶rsel olarak karÅŸÄ±laÅŸtÄ±r"""
    metrics_df = pd.DataFrame({
        'Model': list(results.keys()),
        'CV_Score': [results[m]['cv_score'] for m in results],
        'Test_Accuracy': [results[m]['test_accuracy'] for m in results],
        'Test_F1': [results[m]['test_f1'] for m in results]
    }).sort_values('Test_Accuracy', ascending=False)
    
    print("\n" + "="*60)
    print("ğŸ† MODEL KARÅILAÅTIRMA SONUÃ‡LARI")
    print("="*60)
    print(metrics_df.round(4))
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Accuracy karÅŸÄ±laÅŸtÄ±rmasÄ±
    metrics_df.set_index('Model')[['CV_Score', 'Test_Accuracy']].plot(
        kind='bar', ax=axes[0], color=['skyblue', 'lightcoral']
    )
    axes[0].set_title('Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±\nCV Score vs Test Accuracy')
    axes[0].set_ylabel('Score')
    axes[0].tick_params(axis='x', rotation=45)
    
    # F1-Score karÅŸÄ±laÅŸtÄ±rmasÄ±
    metrics_df.set_index('Model')['Test_F1'].plot(
        kind='bar', ax=axes[1], color='lightgreen'
    )
    axes[1].set_title('Model F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±\n(Weighted)')
    axes[1].set_ylabel('F1-Score')
    axes[1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    return metrics_df

# ---------- 4) EN Ä°YÄ° MODEL DEÄERLENDÄ°RMESÄ° ----------
def evaluate_best_model(best_model, best_model_name, X_test, X_test_scaled, y_test):
    """En iyi modeli detaylÄ± deÄŸerlendir"""
    
    print(f"\nâ­ EN Ä°YÄ° MODEL: {best_model_name}")
    
    # DoÄŸru veri setini seÃ§
    if best_model_name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:
        X_te_eval = X_test_scaled
    else:
        X_te_eval = X_test

    y_pred_best = best_model.predict(X_te_eval)

    print(f"ğŸ“Š Test Accuracy: {accuracy_score(y_test, y_pred_best):.4f}")
    print(f"ğŸ¯ Test F1-Score: {f1_score(y_test, y_pred_best, average='weighted'):.4f}")

    print("\nğŸ“‹ DETAYLI CLASSIFICATION REPORT:")
    print(classification_report(y_test, y_pred_best))

    # Confusion Matrix
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred_best)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=np.unique(y_test), 
                yticklabels=np.unique(y_test))
    plt.title(f'Confusion Matrix - {best_model_name}')
    plt.xlabel('Tahmin Edilen')
    plt.ylabel('GerÃ§ek DeÄŸer')
    plt.tight_layout()
    plt.show()
    
    return y_pred_best

# ---------- 5) FEATURE IMPORTANCE ----------
def plot_feature_importance(best_model, best_model_name, X_train):
    """Feature importance grafiÄŸi Ã§iz"""
    if hasattr(best_model, 'feature_importances_'):
        plt.figure(figsize=(10, 6))
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=True)
        
        plt.barh(feature_importance['feature'], feature_importance['importance'])
        plt.title(f'{best_model_name} - Feature Importance')
        plt.xlabel('Importance')
        plt.tight_layout()
        plt.show()
        
        print("\nğŸ” EN Ã–NEMLÄ° 10 FEATURE:")
        print(feature_importance.tail(10).sort_values('importance', ascending=False))
    else:
        print(f"âš  {best_model_name} modeli feature_importance_ attribute'una sahip deÄŸil")

# ---------- 6) MODEL KAYDETME ----------
def save_pipeline(best_model, scaler, results, file_prefix='crop_soil'):
    """EÄŸitilmiÅŸ pipeline'Ä± kaydet"""
    
    # Modeli kaydet
    model_path = f"{file_prefix}_best_model.pkl"
    joblib.dump(best_model, model_path)
    
    # Scaler'Ä± kaydet
    scaler_path = f"{file_prefix}_scaler.pkl"
    joblib.dump(scaler, scaler_path)
    
    # SonuÃ§larÄ± kaydet
    results_path = f"{file_prefix}_training_results.csv"
    pd.DataFrame(results).T.to_csv(results_path)
    
    print(f"\nğŸ’¾ MODEL KAYDEDÄ°LDÄ°:")
    print(f"   Model: {model_path}")
    print(f"   Scaler: {scaler_path}")
    print(f"   Results: {results_path}")

# ---------- 7) TAHMÄ°N FONKSÄ°YONU ----------
def predict_new_sample(model, scaler, sample_data, feature_names):
    """Yeni Ã¶rnekler iÃ§in tahmin yapar"""
    # DataFrame'e Ã§evir
    sample_df = pd.DataFrame([sample_data], columns=feature_names)
    
    # Scaling gerektiren model mi kontrol et
    model_name = type(model).__name__
    scaling_models = ['LogisticRegression', 'SVC', 'KNeighborsClassifier']
    
    if model_name in scaling_models:
        # SayÄ±sal sÃ¼tunlarÄ± scale et
        num_cols = sample_df.select_dtypes(include=[np.number]).columns
        sample_df[num_cols] = scaler.transform(sample_df[num_cols])
    
    # Tahmin yap
    prediction = model.predict(sample_df)
    probability = model.predict_proba(sample_df) if hasattr(model, 'predict_proba') else None
    
    return prediction[0], probability[0] if probability is not None else None

# ---------- ANA Ä°ÅLEM FONKSÄ°YONU ----------
def main():
    """Ana model eÄŸitimi fonksiyonu"""
    print("ğŸš€ Model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
    
    # Veriyi yÃ¼kle
    df, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = load_cleaned_data()
    
    # Model eÄŸitimi
    results, best_models = train_and_evaluate_models(
        X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, use_scaled=True
    )
    
    # Model karÅŸÄ±laÅŸtÄ±rma
    metrics_comparison = compare_models(results)
    
    # En iyi modeli seÃ§
    best_model_name = metrics_comparison.iloc[0]['Model']
    best_model = best_models[best_model_name]
    
    # En iyi modeli deÄŸerlendir
    y_pred_best = evaluate_best_model(best_model, best_model_name, X_test, X_test_scaled, y_test)
    
    # Feature importance
    plot_feature_importance(best_model, best_model_name, X_train)
    
    # Modeli kaydet
    save_pipeline(best_model, scaler, results, 'crop_soil')
    
    # Ã–rnek tahmin
    print("\nğŸ”® Ã–RNEK TAHMÄ°N:")
    sample_features = {
        'Temperature': 25.0,
        'Humidity': 60.0,
        'Moisture': 45.0,
        'Nitrogen': 20.0,
        'Potassium': 15.0,
        'Phosphorous': 10.0
    }

    # One-hot encoded sÃ¼tunlarÄ± ekle (Ã¶rnek)
    for col in X_train.columns:
        if col not in sample_features and col in ['Soil Type_Sandy', 'Fertilizer Name_Urea']:
            sample_features[col] = 1 if 'Sandy' in col or 'Urea' in col else 0

    prediction, probabilities = predict_new_sample(best_model, scaler, sample_features, X_train.columns)
    print(f"Ã–rnek veri tahmini: {prediction}")
    if probabilities is not None:
        print(f"SÄ±nÄ±f olasÄ±lÄ±klarÄ±: {dict(zip(best_model.classes_, probabilities.round(4)))}")
    
    # SonuÃ§ Ã¶zeti
    print("\n" + "="*60)
    print("âœ… MODELLEME SÃœRECÄ° TAMAMLANDI")
    print("="*60)
    print(f"ğŸ† En Ä°yi Model: {best_model_name}")
    print(f"ğŸ“ˆ Test DoÄŸruluÄŸu: {results[best_model_name]['test_accuracy']:.4f}")
    print(f"ğŸ¯ Test F1-Score: {results[best_model_name]['test_f1']:.4f}")
    print(f"ğŸ“š Toplam Model SayÄ±sÄ±: {len(results)}")
    print(f"ğŸ”¢ EÄŸitim Ã–rnekleri: {X_train.shape[0]}")
    print(f"ğŸ§ª Test Ã–rnekleri: {X_test.shape[0]}")
    print(f"ğŸ¯ SÄ±nÄ±f SayÄ±sÄ±: {len(np.unique(y_train))}")
    print("="*60)

# Program baÅŸlangÄ±cÄ±
if __name__ == "__main__":
    main()